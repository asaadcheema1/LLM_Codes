{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XU85pOubRuky",
        "outputId": "187f7da9-fc02-437b-c737-66ad812ab9e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.1/41.1 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.1/57.1 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.1/320.1 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m58.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.8/63.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "## Necessary for Colab\n",
        "%pip install -q langchain langchain-nvidia-ai-endpoints gradio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"NVIDIA_API_KEY\"] = \"nvapi-Key\"\n",
        "\n",
        "# If you encounter a typing-extensions issue, restart your runtime and try again\n",
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
        "ChatNVIDIA.get_available_models()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vydJ_j_R2fi",
        "outputId": "70002ccd-8301-4e85-a854-f8b1647e11fa"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Model(id='abacusai/dracarys-llama-3.1-70b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='mistralai/mixtral-8x22b-instruct-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-mixtral-8x22b-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='meta/llama-3.1-8b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, base_model=None),\n",
              " Model(id='microsoft/phi-3-medium-128k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-phi-3-medium-128k-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='adept/fuyu-8b', model_type='nv-vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/vlm/adept/fuyu-8b', aliases=['ai-fuyu-8b', 'playground_fuyu_8b', 'fuyu_8b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='microsoft/phi-3.5-moe-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='ibm/granite-3.0-8b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='ibm/granite-34b-code-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-granite-34b-code-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='upstage/solar-10.7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-solar-10_7b-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='google/paligemma', model_type='nv-vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/vlm/google/paligemma', aliases=['ai-google-paligemma'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='nvidia/llama-3.1-nemotron-70b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=True, base_model=None),\n",
              " Model(id='ai21labs/jamba-1.5-large-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='ibm/granite-3.0-3b-a800m-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='microsoft/phi-3-medium-4k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-phi-3-medium-4k-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='google/gemma-2b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-gemma-2b', 'playground_gemma_2b', 'gemma_2b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='nvidia/llama3-chatqa-1.5-70b', model_type='qa', client='ChatNVIDIA', endpoint=None, aliases=['ai-chatqa-1.5-70b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='yentinglin/llama-3-taiwan-70b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='mistralai/mistral-7b-instruct-v0.2', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-mistral-7b-instruct-v2', 'playground_mistral_7b', 'mistral_7b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='meta/codellama-70b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-codellama-70b', 'playground_llama2_code_70b', 'llama2_code_70b', 'playground_llama2_code_34b', 'llama2_code_34b', 'playground_llama2_code_13b', 'llama2_code_13b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='meta/llama3-8b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-llama3-8b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='nvidia/nemotron-4-mini-hindi-4b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=True, base_model=None),\n",
              " Model(id='meta/llama-3.1-405b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, base_model=None),\n",
              " Model(id='microsoft/phi-3-small-128k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-phi-3-small-128k-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='writer/palmyra-med-70b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-palmyra-med-70b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='qwen/qwen2-7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='mistralai/mistral-large', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-mistral-large'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='rakuten/rakutenai-7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='nvidia/nemotron-mini-4b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='google/recurrentgemma-2b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-recurrentgemma-2b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='ai21labs/jamba-1.5-mini-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='microsoft/phi-3-mini-128k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-phi-3-mini'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='mistralai/mistral-large-2-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, base_model=None),\n",
              " Model(id='01-ai/yi-large', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-yi-large'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='meta/llama-3.2-3b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, base_model=None),\n",
              " Model(id='institute-of-science-tokyo/llama-3.1-swallow-8b-instruct-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=True, base_model=None),\n",
              " Model(id='thudm/chatglm3-6b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='nvidia/llama3-chatqa-1.5-8b', model_type='qa', client='ChatNVIDIA', endpoint=None, aliases=['ai-chatqa-1.5-8b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='nvidia/neva-22b', model_type='nv-vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/vlm/nvidia/neva-22b', aliases=['ai-neva-22b', 'playground_neva_22b', 'neva_22b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='ibm/granite-8b-code-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-granite-8b-code-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='microsoft/phi-3-mini-4k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-phi-3-mini-4k', 'playground_phi2', 'phi2'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='meta/llama3-70b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-llama3-70b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='seallms/seallm-7b-v2.5', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-seallm-7b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='mistralai/codestral-22b-instruct-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-codestral-22b-instruct-v01'], supports_tools=False, supports_structured_output=True, base_model=None),\n",
              " Model(id='microsoft/kosmos-2', model_type='nv-vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/vlm/microsoft/kosmos-2', aliases=['ai-microsoft-kosmos-2', 'playground_kosmos_2', 'kosmos_2'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='google/codegemma-1.1-7b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-codegemma-1.1-7b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='meta/llama-3.2-11b-vision-instruct', model_type='vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/gr/meta/llama-3.2-11b-vision-instruct/chat/completions', aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='zyphra/zamba2-7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='meta/llama-3.1-70b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, base_model=None),\n",
              " Model(id='google/deplot', model_type='nv-vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/vlm/google/deplot', aliases=['ai-google-deplot', 'playground_deplot', 'deplot'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='baichuan-inc/baichuan2-13b-chat', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='databricks/dbrx-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-dbrx-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='mistralai/mistral-7b-instruct-v0.3', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-mistral-7b-instruct-v03'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='rakuten/rakutenai-7b-chat', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='meta/llama-3.2-1b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=True, base_model=None),\n",
              " Model(id='google/gemma-2-9b-it', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-gemma-2-9b-it'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='microsoft/phi-3-vision-128k-instruct', model_type='nv-vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/vlm/microsoft/phi-3-vision-128k-instruct', aliases=['ai-phi-3-vision-128k-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='google/gemma-7b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-gemma-7b', 'playground_gemma_7b', 'gemma_7b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='mistralai/mixtral-8x7b-instruct-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-mixtral-8x7b-instruct', 'playground_mixtral_8x7b', 'mixtral_8x7b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='nvidia/llama-3.1-nemotron-51b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='tokyotech-llm/llama-3-swallow-70b-instruct-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='nvidia/mistral-nemo-minitron-8b-8k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=True, base_model=None),\n",
              " Model(id='mistralai/mamba-codestral-7b-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='mediatek/breeze-7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-breeze-7b-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='google/gemma-2-27b-it', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-gemma-2-27b-it'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='writer/palmyra-fin-70b-32k', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=True, base_model=None),\n",
              " Model(id='microsoft/phi-3.5-vision-instruct', model_type='vlm', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='writer/palmyra-med-70b-32k', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-palmyra-med-70b-32k'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='meta/llama-3.2-90b-vision-instruct', model_type='vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/gr/meta/llama-3.2-90b-vision-instruct/chat/completions', aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='deepseek-ai/deepseek-coder-6.7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-deepseek-coder-6_7b-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='institute-of-science-tokyo/llama-3.1-swallow-70b-instruct-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=True, base_model=None),\n",
              " Model(id='mistralai/mathstral-7b-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='microsoft/phi-3-small-8k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-phi-3-small-8k-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='google/codegemma-7b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-codegemma-7b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='nvidia/usdcode-llama3-70b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='snowflake/arctic', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-arctic'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='aisingapore/sea-lion-7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-sea-lion-7b-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='meta/llama2-70b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-llama2-70b', 'playground_llama2_70b', 'llama2_70b', 'playground_llama2_13b', 'llama2_13b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='nv-mistralai/mistral-nemo-12b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, base_model=None),\n",
              " Model(id='google/gemma-2-2b-it', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='nvidia/vila', model_type='vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/vlm/nvidia/vila', aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='microsoft/phi-3.5-mini-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='nvidia/nemotron-4-340b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['qa-nemotron-4-340b-instruct'], supports_tools=False, supports_structured_output=False, base_model=None)]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "## Simple Chat Pipeline\n",
        "chat_llm = ChatNVIDIA(model=\"meta/llama3-8b-instruct\")\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Only respond in rhymes\"),\n",
        "    (\"user\", \"{input}\")\n",
        "])\n",
        "\n",
        "rhyme_chain = prompt | chat_llm | StrOutputParser()\n",
        "\n",
        "print(rhyme_chain.invoke({\"input\" : \"Tell me about Norway!\"}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPZE_4INRw_1",
        "outputId": "acdc4db0-415c-49ff-edf4-aaba7023aa5a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Norway's a land of delight,\n",
            "Where the northern lights shine so bright.\n",
            "The fjords and mountains reach for the sky,\n",
            "And the culture is rich, don't you ask why.\n",
            "\n",
            "The food's delicious, with flavors so fine,\n",
            "Like salmon and waffles, a taste divine.\n",
            "In the summer, the sun stays late,\n",
            "And the midnight sun shines with an radiant state.\n",
            "\n",
            "The people are friendly, with hearts so true,\n",
            "And the language is spoken with an accent or two.\n",
            "From Oslo to Bergen, the cities are grand,\n",
            "With history and culture, in this Scandinavian land.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def rhyme_chat_stream(message, history):\n",
        "    ## This is a generator function, where each call will yield the next entry\n",
        "    buffer = \"\"\n",
        "    for token in rhyme_chain.stream({\"input\" : message}):\n",
        "        buffer += token\n",
        "        yield buffer\n",
        "\n",
        "## Uncomment when you're ready to try this.\n",
        "demo = gr.ChatInterface(rhyme_chat_stream).queue()\n",
        "window_kwargs = {} # or {\"server_name\": \"0.0.0.0\", \"root_path\": \"/7860/\"}\n",
        "demo.launch(share=True, debug=True, **window_kwargs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660
        },
        "id": "AT1zvrlSSySh",
        "outputId": "ce456ca3-38bc-4b12-a72b-b276fc64f20b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gradio/components/chatbot.py:243: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://3bf6caeeebc827e881.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://3bf6caeeebc827e881.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://3bf6caeeebc827e881.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Internal Response (Classification with zero shot)"
      ],
      "metadata": {
        "id": "lUfwSAi5TveM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "## https://build.nvidia.com\n",
        "instruct_llm = ChatNVIDIA(model=\"mistralai/mistral-7b-instruct-v0.3\")\n",
        "\n",
        "sys_msg = (\n",
        "    \"Choose the most likely topic classification given the sentence as context.\"\n",
        "    \" Only one word, no explanation.\\n[Options : {options}]\"\n",
        ")\n",
        "\n",
        "## One-shot classification prompt with heavy format assumptions.\n",
        "zsc_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", sys_msg),\n",
        "    (\"user\", \"[[The sea is awesome]]\"),\n",
        "    (\"assistant\", \"boat\"),\n",
        "    (\"user\", \"[[{input}]]\"),\n",
        "])\n",
        "\n",
        "zsc_chain = zsc_prompt | instruct_llm | StrOutputParser()\n",
        "\n",
        "def zsc_call(input, options=[\"car\", \"boat\", \"airplane\", \"bike\"]):\n",
        "    return zsc_chain.invoke({\"input\" : input, \"options\" : options}).split()[0]\n",
        "\n",
        "print(\"-\" * 80)\n",
        "print(zsc_call(\"Should I take the next exit, or keep going to the next one?\"))\n",
        "\n",
        "print(\"-\" * 80)\n",
        "print(zsc_call(\"I get seasick, so I think I'll pass on the trip\"))\n",
        "\n",
        "print(\"-\" * 80)\n",
        "print(zsc_call(\"I'm scared of heights, so flying probably isn't for me\"))\n",
        "\n",
        "print(\"-\" * 80)\n",
        "print(zsc_call(\"The Road is under Construction\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22z0EXfaTsIy",
        "outputId": "559d75b1-d80f-4e86-9c7b-b04a3da4798f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "boat\n",
            "--------------------------------------------------------------------------------\n",
            "boat\n",
            "--------------------------------------------------------------------------------\n",
            "airplane\n",
            "--------------------------------------------------------------------------------\n",
            "bike\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from copy import deepcopy\n",
        "\n",
        "instruct_llm = ChatNVIDIA(model=\"mistralai/mixtral-8x22b-instruct-v0.1\")  ## Feel free to change the models\n",
        "\n",
        "prompt1 = ChatPromptTemplate.from_messages([(\"user\", (\n",
        "    \"INSTRUCTION: Only respond in rhymes\"\n",
        "    \"\\n\\nPROMPT: {input}\"\n",
        "))])\n",
        "\n",
        "prompt2 =  ChatPromptTemplate.from_messages([(\"user\", (\n",
        "    \"INSTRUCTION: Only responding in rhyme, change the topic of the input poem to be about {topic}!\"\n",
        "    \" Make it happy! Try to keep the same sentence structure, but make sure it's easy to recite!\"\n",
        "    \" Try not to rhyme a word with itself.\"\n",
        "    \"\\n\\nOriginal Poem: {input}\"\n",
        "    \"\\n\\nNew Topic: {topic}\"\n",
        "))])\n",
        "\n",
        "## These are the main chains, constructed here as modules of functionality.\n",
        "chain1 = prompt1 | instruct_llm | StrOutputParser()  ## only expects input\n",
        "chain2 = prompt2 | instruct_llm | StrOutputParser()  ## expects both input and topic"
      ],
      "metadata": {
        "id": "FvKQMzWpXBhF"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rhyme_chat2_stream(message, history, return_buffer=True):\n",
        "    '''This is a generator function, where each call will yield the next entry'''\n",
        "\n",
        "    first_poem = None\n",
        "    for entry in history:\n",
        "        if entry[0] and entry[1]:\n",
        "            ## If a generation occurred as a direct result of a user input,\n",
        "            ##  keep that response (the first poem generated) and break out\n",
        "            first_poem = entry[1]\n",
        "            break\n",
        "\n",
        "    if first_poem is None:\n",
        "        ## First Case: There is no initial poem generated. Better make one up!\n",
        "\n",
        "        buffer = \"Oh! I can make a wonderful poem about that! Let me think!\\n\\n\"\n",
        "        yield buffer\n",
        "\n",
        "        ## iterate over stream generator for first generation\n",
        "        inst_out = \"\"\n",
        "        chat_gen = chain1.stream({\"input\" : message})\n",
        "        for token in chat_gen:\n",
        "            inst_out += token\n",
        "            buffer += token\n",
        "            yield buffer if return_buffer else token\n",
        "\n",
        "        passage = \"\\n\\nNow let me rewrite it with a different focus! What should the new focus be?\"\n",
        "        buffer += passage\n",
        "        yield buffer if return_buffer else passage\n",
        "\n",
        "    else:\n",
        "        ## Subsequent Cases: There is a poem to start with. Generate a similar one with a new topic!\n",
        "\n",
        "        # yield f\"Not Implemented!!!\"; return ## <- TODO: Comment this out\n",
        "\n",
        "        ########################################################################\n",
        "        ## TODO: Invoke the second chain to generate the new rhymes.\n",
        "\n",
        "        buffer = f\"Sure! Here you go!\\n\\n\"\n",
        "        yield buffer\n",
        "\n",
        "        ## iterate over stream generator for second generation\n",
        "        chat_gen = chain2.stream({\"input\" : first_poem, \"topic\" : message})\n",
        "        for token in chat_gen:\n",
        "            buffer += token\n",
        "            yield buffer if return_buffer else token\n",
        "\n",
        "        ## END TODO\n",
        "        ########################################################################\n",
        "\n",
        "        passage = \"\\n\\nThis is fun! Give me another topic!\"\n",
        "        buffer += passage\n",
        "        yield buffer if return_buffer else passage\n",
        "\n",
        "################################################################################"
      ],
      "metadata": {
        "id": "oKdtf4pAXHpr"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def queue_fake_streaming_gradio(chat_stream, history = [], max_questions=3):\n",
        "\n",
        "    ## Mimic of the gradio initialization routine, where a set of starter messages can be printed off\n",
        "    for human_msg, agent_msg in history:\n",
        "        if human_msg: print(\"\\n[ Human ]:\", human_msg)\n",
        "        if agent_msg: print(\"\\n[ Agent ]:\", agent_msg)\n",
        "\n",
        "    ## Mimic of the gradio loop with an initial message from the agent.\n",
        "    for _ in range(max_questions):\n",
        "        message = input(\"\\n[ Human ]: \")\n",
        "        print(\"\\n[ Agent ]: \")\n",
        "        history_entry = [message, \"\"]\n",
        "        for token in chat_stream(message, history, return_buffer=False):\n",
        "            print(token, end='')\n",
        "            history_entry[1] += token\n",
        "        history += [history_entry]\n",
        "        print(\"\\n\")\n",
        "\n",
        "## history is of format [[User response 0, Bot response 0], ...]\n",
        "history = [[None, \"Let me help you make a poem! What would you like for me to write?\"]]\n",
        "\n",
        "## Simulating the queueing of a streaming gradio interface, using python input\n",
        "queue_fake_streaming_gradio(\n",
        "    chat_stream = rhyme_chat2_stream,\n",
        "    history = history\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1lXm5KlsXTIM",
        "outputId": "1c75af70-ca0f-4090-bf0e-5142c9e9f3b9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[ Agent ]: Let me help you make a poem! What would you like for me to write?\n",
            "\n",
            "[ Human ]: Life\n",
            "\n",
            "[ Agent ]: \n",
            "Oh! I can make a wonderful poem about that! Let me think!\n",
            "\n",
            "In the grand scheme, life's a dream, a blend of joy and sorrow's stream.\n",
            "At times it's tough, but we're tougher, through the huff and puff we buffer.\n",
            "In every dawn, a chance to grow, in the dusk, a tale to show.\n",
            "Life's a canvas, paint it bright, with love, with laughter, day and night.\n",
            "In every moment, there's a gleam, life's a wondrous, grand scheme.\n",
            "\n",
            "Now let me rewrite it with a different focus! What should the new focus be?\n",
            "\n",
            "\n",
            "[ Human ]: Love\n",
            "\n",
            "[ Agent ]: \n",
            "Sure! Here you go!\n",
            "\n",
            "Oh! I can craft a joyful verse about love, bring it to life, up above!\n",
            "Love, our grand theme, brighter than a dream, a beacon of joy, a warmth supreme.\n",
            "At times it's tough, but we're tougher together, through the huff and puff, we're with each other.\n",
            "In every dawn, a chance to show, in the dusk, a love will grow.\n",
            "Love is our canvas, paint it bright, with laughter, in day and night.\n",
            "In every moment, there's a gleam, love's a wondrous, grand scheme.\n",
            "\n",
            "This is fun! Give me another topic!\n",
            "\n",
            "\n",
            "[ Human ]: Uiversity\n",
            "\n",
            "[ Agent ]: \n",
            "Sure! Here you go!\n",
            "\n",
            "In the halls of learning, knowledge gleams, a fertile ground for brightest dreams!\n",
            "Oh, studies can be tough, yet we're tougher, with perseverance we buffer.\n",
            "With every class, we grow and sow, at sunset, wisdom gladly we'll bestow.\n",
            "University's a canvas vast, filled with knowledge that forever lasts.\n",
            "In every lecture, there's a gleam, of future progress and grand scheme.\n",
            "\n",
            "And now, dear friends, please join and rhyme, let's share the joy of learning's prime!\n",
            "\n",
            "This is fun! Give me another topic!\n",
            "\n"
          ]
        }
      ]
    }
  ]
}